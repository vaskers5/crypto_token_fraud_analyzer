# src/services/combined_inspector.py

import json
import asyncio
import requests
import xml.etree.ElementTree as ET
from datetime import datetime
from typing import Dict, Any, Optional, List

import pandas as pd

from src.services.token_data_fetcher import TokenDataFetcher
from src.services.boosting_classifier import BoostingFraudClassifier
from src.models.gemini_wrapper import GeminiWrapper
from src.config.settings import FRAUD_MODEL_PATH, SUPPORTED_CHAINS_PATH


class CombinedTokenInspector:
    NEWS_RSS_URL = "https://news.google.com/rss/search?q={q}&hl=ru&gl=RU&ceid=RU:ru"

    def __init__(self):
        # –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Ç–∏–≤–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        with open(SUPPORTED_CHAINS_PATH, encoding="utf-8") as f:
            chains = json.load(f)
        self.native_tokens = {
            entry["native_symbol"].lower(): entry["id"]
            for entry in chains if entry.get("native_symbol")
        }

        self.fetcher    = TokenDataFetcher()
        self.classifier = BoostingFraudClassifier(FRAUD_MODEL_PATH)
        self.gemini     = GeminiWrapper()

    def _fetch_news(self, query: str, max_items: int = 5) -> List[Dict[str, str]]:
        url = self.NEWS_RSS_URL.format(q=requests.utils.requote_uri(query))
        try:
            resp = requests.get(url, timeout=5)
            resp.raise_for_status()
            root = ET.fromstring(resp.content)
            items = root.findall('.//item')[:max_items]
            news = []
            for itm in items:
                title = itm.findtext('title', '').strip()
                pub   = itm.findtext('pubDate', '').strip()
                news.append({'date': pub, 'title': title})
            return news
        except Exception:
            return []

    def _format_bullets(self, items: List[str]) -> str:
        return "\n".join(f"‚Ä¢ {line}" for line in items)

    def _build_final_prompt(
        self,
        symbol: str,
        date_str: str,
        news: List[Dict[str, str]],
        risk_context: Optional[str],
        analysis_items: List[str]
    ) -> str:
        # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏
        news_lines = [f"{n['date']} ‚Äî {n['title']}" for n in news]

        # –°—Ç–∞—Ä—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
        parts = [
            f"–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ç–æ–∫–µ–Ω—É {symbol}:",
            f"–î–∞—Ç–∞: {date_str}",
        ]
        if risk_context:
            parts.append(risk_context.strip())
        if news_lines:
            parts.append("–ù–æ–≤–æ—Å—Ç–∏:")
            parts.extend(news_lines)
        if analysis_items:
            parts.append("–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ –º–µ—Ç—Ä–∏–∫–∏:")
            parts.extend(analysis_items)

        # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –∏—Ç–æ–≥–æ–≤–æ–º—É –æ—Ç—á—ë—Ç—É
        instruction = (
            "–°–æ—Å—Ç–∞–≤—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á—ë—Ç —Ç–æ–ª—å–∫–æ –ø–æ —Ç–æ–∫–µ–Ω—É {symbol}, –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ç—Ä–æ–≥–æ —Å–ª–µ–¥—É—é—â–µ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:\n"
            "‚Ä¢ –î–ª—è –æ–±—ã—á–Ω—ã—Ö –ø—É–Ω–∫—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π —Å–∏–º–≤–æ–ª ‚Ä¢\n"
            "‚Ä¢ –î–ª—è –∂–∏—Ä–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–π *—Ç–µ–∫—Å—Ç*\n\n"
            "üí° *–û–°–ù–û–í–ù–´–ï –í–´–í–û–î–´:*\n"
            "*–ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –æ —Ç–æ–∫–µ–Ω–µ:*\n"
            "‚Ä¢ –°–∏–º–≤–æ–ª –∏ –∞–¥—Ä–µ—Å –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)\n"
            "‚Ä¢ –ß–∏—Å–ª–æ –ª–∏—Å—Ç–∏–Ω–≥–æ–≤ –∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ)\n"
            "*–í–∞–∂–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:*\n"
            "‚Ä¢ –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ –∏ —Ä—ã–Ω–∫–∞\n"
            "‚ö†Ô∏è *–£–†–û–í–ï–ù–¨ –†–ò–°–ö–ê:*\n"
            "‚Ä¢ –û—Ü–µ–Ω–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å–∫–∞–º–∞ –∏ red flags\n"
            "üéØ *–í–ï–†–î–ò–ö–¢:*\n"
            "‚Ä¢ –°–∫–∞–º/–Ω–µ —Å–∫–∞–º\n"
            "üëâ *–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:*\n"
            "‚Ä¢ –ò–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å/–Ω–µ –∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∏—Å–∫–∞\n\n"
            "–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –¥—Ä—É–≥–∏–µ —Å–∏–º–≤–æ–ª—ã —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫—Ä–æ–º–µ ‚Ä¢ –∏ *."
        ).format(symbol=symbol)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º
        prompt = "\n".join(parts + [instruction])
        return prompt

    async def inspect(
        self,
        symbol: str,
        chain: Optional[str] = None
    ) -> Dict[str, Any]:
        symbol_u = symbol.upper()
        symbol_l = symbol.lower()

        # –î–∞—Ç–∞
        months = {
            1: "—è–Ω–≤–∞—Ä—è", 2: "—Ñ–µ–≤—Ä–∞–ª—è", 3: "–º–∞—Ä—Ç–∞", 4: "–∞–ø—Ä–µ–ª—è",
            5: "–º–∞—è", 6: "–∏—é–Ω—è", 7: "–∏—é–ª—è", 8: "–∞–≤–≥—É—Å—Ç–∞",
            9: "—Å–µ–Ω—Ç—è–±—Ä—è", 10: "–æ–∫—Ç—è–±—Ä—è", 11: "–Ω–æ—è–±—Ä—è", 12: "–¥–µ–∫–∞–±—Ä—è"
        }
        now = datetime.now()
        date_str = f"{now.day} {months[now.month]} {now.year}"

        # –ù–æ–≤–æ—Å—Ç–∏
        news = self._fetch_news(symbol_u)

        # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –Ω–∞—Ç–∏–≤–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        is_native = symbol_l in self.native_tokens
        risk_context = None
        if is_native:
            network = self.native_tokens[symbol_l]
            risk_context = f"*–í–∞–∂–Ω–∞—è –∑–∞–º–µ—Ç–∫–∞:* {symbol_u} ‚Äî –Ω–∞—Ç–∏–≤–Ω—ã–π —Ç–æ–∫–µ–Ω —Å–µ—Ç–∏ {network}, —Ä–∏—Å–∫–∏ –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Å–µ—Ç–∏.\n"

        if is_native:
            analysis_items = []
            # –æ—Å–Ω–æ–≤–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
            analysis_items.append("–ù–∞—Ç–∏–≤–Ω—ã–π —Ç–æ–∫–µ–Ω ‚Äî —Ä–∏—Å–∫–∏ –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Å–µ—Ç–∏.")
            prompt = self._build_final_prompt(symbol_u, date_str, news, risk_context, analysis_items)
            llm_report = await asyncio.get_running_loop().run_in_executor(
                None, lambda: self.gemini.generate(prompt)
            )
            return {
                'symbol': symbol_u,
                'prediction': False,
                'scam_probability': 0.01,
                'llm_report': llm_report
            }

        # –ü–ª–∞—Ç—Ñ–æ—Ä–º—ã –∏ –∞–¥—Ä–µ—Å
        try:
            platforms = self.fetcher.get_token_platforms(symbol_u)
        except Exception:
            platforms = {}
        address = None
        if platforms:
            address = platforms.get(chain) or next(iter(platforms.values()))

        # –§–∏—á–∏ –∏ –ø—Ä–æ–≥–Ω–æ–∑
        features, is_scam, scam_prob = {}, None, None
        if address:
            try:
                features = self.fetcher.get_token_features(address)
                df = pd.DataFrame([features])
                is_scam = bool(self.classifier.predict(df).iloc[0])
                scam_prob = float(
                    self.classifier.predict_proba(df)[f"prob_class_{int(is_scam)}"].iloc[0]
                ) * 100
            except Exception:
                pass

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑
        analysis_items = []
        if address:
            analysis_items.append(f"–ê–¥—Ä–µ—Å: {address}")
        for k, v in (features.items()):
            analysis_items.append(f"{k.replace('_', ' ')}: {v}")
        if scam_prob is not None:
            analysis_items.append(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–∫–∞–º–∞: {scam_prob:.1f}%")

        # –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        prompt = self._build_final_prompt(symbol_u, date_str, news, risk_context, analysis_items)
        llm_report = await asyncio.get_running_loop().run_in_executor(
            None, lambda: self.gemini.generate(prompt)
        )

        result = {
            'symbol': symbol_u,
            'prediction': is_scam,
            'scam_probability': scam_prob,
            'llm_report': llm_report
        }
        if address:
            result['address'] = address
        return result
