import logging
from telegram import Update
from telegram.ext import (
    ApplicationBuilder,
    CommandHandler,
    MessageHandler,
    filters,
    ContextTypes,
    ConversationHandler,
)
from .config.settings import TG_BOT_TOKEN
from .services.token_analyzer import TokenAnalyzer
from .api.coingecko import TokenNotFoundError, NativeTokenError  # –¥–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç

WAIT_TICKER = 0

class ScamAnalyzerBot:
    def __init__(self):
        self.token_analyzer = TokenAnalyzer()
        self.logger = logging.getLogger(__name__)

    def run(self):
        """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞."""
        app = ApplicationBuilder().token(TG_BOT_TOKEN).build()
        
        conv = ConversationHandler(
            entry_points=[CommandHandler('start', self.start)],
            states={
                WAIT_TICKER: [
                    MessageHandler(
                        filters.TEXT & ~filters.COMMAND, 
                        self.handle_ticker
                    )
                ],
            },
            fallbacks=[CommandHandler('cancel', self.cancel)],
        )
        
        app.add_handler(conv)
        app.run_polling()

    @staticmethod
    def escape_markdown(text: str) -> str:
        """–≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è MarkdownV2."""
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –∂–∏—Ä–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        import re
        
        def escape_special_chars(text: str) -> str:
            chars = ['_', '*', '[', ']', '(', ')', '~', '`', '>', '#', '+', 
                    '-', '=', '|', '{', '}', '.', '!']
            for char in chars:
                text = text.replace(char, f'\\{char}')
            return text
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —á–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ –º–µ–∂–¥—É –∑–≤–µ–∑–¥–æ—á–∫–∞–º–∏
        parts = []
        last_end = 0
        
        for match in re.finditer(r'\*(.*?)\*', text):
            start, end = match.span()
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –¥–æ –∑–≤–µ–∑–¥–æ—á–µ–∫
            if start > last_end:
                parts.append(escape_special_chars(text[last_end:start]))
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∂–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç
            bold_content = escape_special_chars(match.group(1))
            parts.append(f'*{bold_content}*')
            
            last_end = end
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–π—Å—è —Ç–µ–∫—Å—Ç
        if last_end < len(text):
            parts.append(escape_special_chars(text[last_end:]))
        
        return ''.join(parts)

    async def start(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
        await update.message.reply_text(
            "–í–≤–µ–¥–∏—Ç–µ —Ç–∏–∫–µ—Ä —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, BTC, ETH, BNB):"
        )
        return WAIT_TICKER

    async def handle_ticker(
        self, 
        update: Update, 
        context: ContextTypes.DEFAULT_TYPE
    ) -> int:
        symbol = update.message.text.strip().upper()
        await update.message.reply_text(
            "–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–æ–∫–µ–Ω, —ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è..."
        )

        try:
            token_info = await self.token_analyzer.get_token_info(symbol)
            gemini_results, final_analysis = await self.token_analyzer.analyze_token(
                token_info
            )
            
            message = (
                f"üîç *–ê–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–∞ {self.escape_markdown(symbol)}*\n\n"
                f"{self.escape_markdown(final_analysis)}"
            )
            
            if len(message) > 4096:
                parts = [message[i:i+4096] for i in range(0, len(message), 4096)]
                for part in parts:
                    await update.message.reply_text(
                        part, 
                        parse_mode='MarkdownV2'
                    )
            else:
                await update.message.reply_text(
                    message, 
                    parse_mode='MarkdownV2'
                )
            
            return ConversationHandler.END

        except NativeTokenError as e:  # –¥–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –Ω–æ–≤–æ–π –æ—à–∏–±–∫–∏
            await update.message.reply_text(
                self.escape_markdown(str(e)),
                parse_mode='MarkdownV2'
            )
            return ConversationHandler.END
        except TokenNotFoundError as e:
            await update.message.reply_text(
                self.escape_markdown(str(e)), 
                parse_mode='MarkdownV2'
            )
            return ConversationHandler.END
        except Exception as e:
            self.logger.exception("Unexpected error during analysis")
            await update.message.reply_text(
                self.escape_markdown(
                    "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
                ),
                parse_mode='MarkdownV2'
            )
            return ConversationHandler.END

    async def cancel(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
        await update.message.reply_text("–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞.")
        return ConversationHandler.END