Погнали! Ниже черновик «презенташки» — говорим простыми словами, без канцелярита, но чтобы любой понял, что делает бот и зачем к нему прикручивать LLM.

---

## Кто мы такие и зачем всё это  
Наш телеграм‑бот помогает быстро пробить токен: вводишь тикер, выбираешь сеть и получаешь адрес контракта плюс вердикт «скам / не скам». Бот дергает CoinGecko, кэширует ответы локально, не грузит пользователя сложной аналитикой и работает буквально за пару секунд. citeturn0search0turn0search7

## Как это уже работает под капотом  
1. **Поиск токена.** Через `/search` эндпоинт CoinGecko находим ID монеты, а потом вытаскиваем список платформ → адресов контрактов. citeturn0search0turn0search7  
2. **Кэш.** Кидаем всё в `shelve`‑базу, чтобы не бомбить API и ускорить ответы. citeturn0search2  
3. **Диалог.** Используем `python‑telegram‑bot` + `ConversationHandler`, поэтому бот ведёт нормальную многошаговую беседу. citeturn0search1  
4. **Подсказки по сетям.** Если юзер опечатался, `difflib.get_close_matches` предлагает похожие ID сетей. citeturn0search3  
5. **Вердикт сейчас игрушечный** — рандом (кроме твикса с USDT и LUNA). Это самый слабый кусок, который и просит LLM.

## Боли, которые надо лечить  
* 25 % новых токенов в 2022 — банальные pump‑and‑dump схемы. Люди теряют деньги, потому что не могут быстро проверить риск. citeturn0news104  
* Уже есть исследования, где трансформеры ловят скам‑токены по ончейн‑паттернам и соцсетям («DeFiTrust»). citeturn0search5  
* При этом LLM‑ы умеют дружить с Телеграмом через function‑calling и делать ответы человечными. citeturn0search4turn0search6  

## Куда впихнуть LLM и зачем  

### 1. Умный анти‑скам вместо рандома  
Берём описание токена, код контракта (если open‑source), свежие твиты/реддит и прогоняем через LLM‑классификатор. Он отвечает «low / medium / high risk» и поясняет, что именно насторожило: мало ликвидности, странные роли в контракте, агрессивный маркетинг и т.д. citeturn0search5turn0search8  

### 2. Нормальные человекопонятные пояснения  
LLM переводит сухие данные CoinGecko (market cap, supply, графики) на язык «для людей»:  
> «У монеты капа как у среднего DeFi‑стартапа, объемы скромные, держателей мало — будь осторожен!»  
Таким образом бот не просто «выдаёт цифры», а рассказывает, что они значат. citeturn0search4  

### 3. Авто‑обработка опечаток и мультиязычность  
LLM умеет понимать «биткоин», «btc», «биткоин на эфире» и другие варианты, сразу выдавая релевантный результат, плюс переводит ответы на язык пользователя. citeturn0search4  

### 4. Голосовые и inline‑режим  
Можно заставить LLM парсить голосовые сообщения («Проверь мне USDC на арбитруме») и отвечать прямо в чате без /start. citeturn0search6  

### 5. Защита самого бота  
LLM поможет фильтровать prompt‑инъекции, чтобы никто не заставил его скинуть токен «секретные ключи». citeturn0news106turn0search9  

## Архитектура с LLM (быстро)  
```
Пользователь → Telegram Bot → 
   ① Проверка CoinGecko (адреса) →  
   ② LLM‑«Risk Engine» (ончейн + соцсети) →  
   ③ LLM‑«Explainer» (нормальный текст) →  
Ответ юзеру
```  

* Все LLM‑компоненты можно хостить через OpenAI API или локальный Llama‑дериват, если важна приватность.  
* Для ончейн‑данных берём публичные RPC или TheGraph.  
* Для соцсетей — кэшируем твиты/посты, чтобы не нарушать rate limit.  

## Дорожная карта  
| Этап | Что делаем | Пример пользы |
|------|-----------|---------------|
| MVP‑2 | Вырезаем рандом, подключаем LLM‑классификатор | Реальный риск‑скор вместо «монетка или пушка» |
| Q3‑2025 | Появится голосовой ввод + мультиязычность | Пользователь из Бразилии задаёт вопрос по‑португальски |
| Q4‑2025 | Публичный дашборд c рейтингами токенов | Можно шерить ссылку друзьям |
| 2026 | DAO‑режим: модель учится на фидбэке сообщества | Самообучение и более точные прогнозы |

